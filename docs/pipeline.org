** Config

I start off by defining some absolute directories that will be referred to throughout the script.  


#+BEGIN_SRC yaml :tangle '("../workflow/config_base.yaml" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/config_base.yaml" "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/config_base.yaml")
  ---
  flag_file: &hst !Host {options: {midway2: "/project2", gardner: "/gpfs/data/xhe-lab/", desktop: "/run/media/nwknoblauch/Data"} }
  paths: 
    'DL': &dl  !Dep {host: *hst, pref:  null, path: { midway2: "/project2/xinhe/", gardner: "/gpfs/data/xhe-lab/", desktop: "/run/media/nwknoblauch/Data/"}}
    '1KG':     !Dep {host: *hst, pref: *dl, path: {midway2: &1kg "1kg/", gardner: *1kg , desktop: *1kg} }
    'BED':     !Dep {host: *hst, pref: *dl, path: {midway2: &bed "genomic_annotation/ptb_epigenetic/", gardner: *bed, desktop: "ptb_scratch/new_bed/"}}
    'L2':      !Dep {host: *hst, pref: *dl, path: {midway2: &l2 "genomic_annotation/L2/", gardner: *l2, desktop: "L2/"}}
    'ANNO':    !Dep {host: *hst, pref: *dl, path: {midway2: *l2 , gardner: *l2, desktop: *l2}}
    'WEIGHTS': !Dep {host: *hst, pref: *dl, path: {midway2: &weight "1kg/1000G_Phase3_weights_hm3_no_MHC/", gardner: *weight, desktop: *weight}} 
    'FRQF':    !Dep {host: *hst, pref: *dl, path: {midway2: &frq "1kg/1000G_Phase3_frq/", gardner: *frq, desktop: "1kg/1000G_Phase3_frq/"}} 
    'GWAS':    !Dep {host: *hst, pref: *dl, path: {midway2: &gwas "ptb/", gardner: *gwas, desktop: "gwas_data/gwas_sumstats/"}}
  envs:
    'r':    !Dep {host: *hst, pref:  null, path: { midway2: null, gardner: null, desktop: null }}
    'ldsc': !Dep {host: *hst, pref:  null, path: { midway2: null, gardner: null, desktop: "../envs/ldsc.yml" }}
    'cmd_prefix': !Dep {host: *hst, pref:  null, path: { midway2: ". /project2/xinhe/software/ldsc/workflow/ldsc_env.sh; ", gardner: ". /gpfs/data/xhe-lab/software/ldsc/workflow/ldsc_env.sh;  ", desktop: "" }}

#+END_SRC

#+BEGIN_SRC yaml :tangle '("../workflow/cluster_config.yaml" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/cluster_config.yaml" "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/cluster_config.yaml")
---
__default__:
  partition: 'broadwl'
  time: '01:00:00'
  nodes: '1'
  ntasks: '1'
  cpuspertask: '1'
  mem: '25gb'
run_ldsc:
  mem: '45gb'
  time: '02:00:00'


#+END_SRC



#+BEGIN_SRC yaml :tangle '("../workflow/base_model.yaml" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/base_model.yaml"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/base_model.yaml" )
- 'Coding_UCSCL2'
- 'Coding_UCSC.flanking.500L2'
- 'Conserved_LindbladTohL2'
- 'Conserved_LindbladToh.flanking.500L2'
- 'CTCF_HoffmanL2'
- 'CTCF_Hoffman.flanking.500L2'
- 'DGF_ENCODEL2'
- 'DGF_ENCODE.flanking.500L2'
- 'DHS_peaks_TrynkaL2'
- 'DHS_TrynkaL2'
- 'DHS_Trynka.flanking.500L2'
- 'Enhancer_AnderssonL2'
- 'Enhancer_Andersson.flanking.500L2'
- 'Enhancer_HoffmanL2'
- 'Enhancer_Hoffman.flanking.500L2'
- 'FetalDHS_TrynkaL2'
- 'FetalDHS_Trynka.flanking.500L2'
- 'H3K27ac_HniszL2'
- 'H3K27ac_Hnisz.flanking.500L2'
- 'H3K27ac_PGC2L2'
- 'H3K27ac_PGC2.flanking.500L2'
- 'H3K4me1_peaks_TrynkaL2'
- 'H3K4me1_TrynkaL2'
- 'H3K4me1_Trynka.flanking.500L2'
- 'H3K4me3_peaks_TrynkaL2'
- 'H3K4me3_TrynkaL2'
- 'H3K4me3_Trynka.flanking.500L2'
- 'H3K9ac_peaks_TrynkaL2'
- 'H3K9ac_TrynkaL2'
- 'H3K9ac_Trynka.flanking.500L2'
- 'Intron_UCSCL2'
- 'Intron_UCSC.flanking.500L2'
- 'PromoterFlanking_HoffmanL2'
- 'PromoterFlanking_Hoffman.flanking.500L2'
- 'Promoter_UCSCL2'
- 'Promoter_UCSC.flanking.500L2'
- 'Repressed_HoffmanL2'
- 'Repressed_Hoffman.flanking.500L2'
- 'SuperEnhancer_HniszL2'
- 'SuperEnhancer_Hnisz.flanking.500L2'
- 'TFBS_ENCODEL2'
- 'TFBS_ENCODE.flanking.500L2'
- 'Transcr_HoffmanL2'
- 'Transcr_Hoffman.flanking.500L2'
- 'TSS_HoffmanL2'
- 'TSS_Hoffman.flanking.500L2'
- 'UTR_3_UCSCL2'
- 'UTR_3_UCSC.flanking.500L2'
- 'UTR_5_UCSCL2'
- 'UTR_5_UCSC.flanking.500L2'
- 'WeakEnhancer_HoffmanL2'
- 'WeakEnhancer_Hoffman.flanking.500L2'
- 'GERP.NSL2'
- 'GERP.RSsup4L2'
- 'MAFbin1L2'
- 'MAFbin2L2'
- 'MAFbin3L2'
- 'MAFbin4L2'
- 'MAFbin5L2'
- 'MAFbin6L2'
- 'MAFbin7L2'
- 'MAFbin8L2'
- 'MAFbin9L2'
- 'MAFbin10L2'
- 'MAF_Adj_Predicted_Allele_AgeL2'
- 'MAF_Adj_LLD_AFRL2'
- 'Recomb_Rate_10kbL2'
- 'Nucleotide_Diversity_10kbL2'
- 'Backgrd_Selection_StatL2'
- 'CpG_Content_50kbL2'
- 'MAF_Adj_ASMCL2'
- 'GTEx_eQTL_MaxCPPL2'
- 'BLUEPRINT_H3K27acQTL_MaxCPPL2'
- 'BLUEPRINT_H3K4me1QTL_MaxCPPL2'
- 'BLUEPRINT_DNA_methylation_MaxCPPL2'
- 'synonymousL2'
- 'non_synonymousL2'
- 'Conserved_Vertebrate_phastCons46wayL2'
- 'Conserved_Vertebrate_phastCons46way.flanking.500L2'
- 'Conserved_Mammal_phastCons46wayL2'
- 'Conserved_Mammal_phastCons46way.flanking.500L2'
- 'Conserved_Primate_phastCons46wayL2'
- 'Conserved_Primate_phastCons46way.flanking.500L2'
- 'BivFlnkL2'
- 'BivFlnk.flanking.500L2'
- 'Human_Promoter_VillarL2'
- 'Human_Promoter_Villar.flanking.500L2'
- 'Human_Enhancer_VillarL2'
- 'Human_Enhancer_Villar.flanking.500L2'
- 'Ancient_Sequence_Age_Human_PromoterL2'
- 'Ancient_Sequence_Age_Human_Promoter.flanking.500L2'
- 'Ancient_Sequence_Age_Human_EnhancerL2'
- 'Ancient_Sequence_Age_Human_Enhancer.flanking.500L2'
- 'Human_Enhancer_Villar_Species_Enhancer_CountL2'
- 'Human_Promoter_Villar_ExACL2'
- 'Human_Promoter_Villar_ExAC.flanking.500L2'
#+END_SRC


#+BEGIN_SRC yaml :tangle '("../workflow/annots.yaml" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/annots.yaml"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/annots.yaml" )
  ---
  baseline_model: 
    - Coding_UCSCL2
    - Conserved_LindbladTohL2
    - CTCF_HoffmanL2
    - DGF_ENCODEL2
    - DHS_peaks_TrynkaL2
    - DHS_TrynkaL2
    - Enhancer_AnderssonL2
    - Enhancer_HoffmanL2
    - FetalDHS_TrynkaL2
    - H3K27ac_HniszL2
    - H3K27ac_PGC2L2
    - H3K4me1_peaks_TrynkaL2
    - H3K4me1_TrynkaL2
    - H3K4me3_peaks_TrynkaL2
    - H3K4me3_TrynkaL2
    - H3K9ac_peaks_TrynkaL2
    - H3K9ac_TrynkaL2
    - Intron_UCSCL2
    - PromoterFlanking_HoffmanL2
    - Promoter_UCSCL2
    - Repressed_HoffmanL2
    - SuperEnhancer_HniszL2
    - TFBS_ENCODEL2
    - Transcr_HoffmanL2
    - TSS_HoffmanL2
    - UTR_3_UCSCL2
    - UTR_5_UCSCL2
    - WeakEnhancer_HoffmanL2
    - GERP.NSL2
    - GERP.RSsup4L2
    - MAFbin1L2
    - MAFbin2L2
    - MAFbin3L2
    - MAFbin4L2
    - MAFbin5L2
    - MAFbin6L2
    - MAFbin7L2
    - MAFbin8L2
    - MAFbin9L2
    - MAFbin10L2
    - MAF_Adj_Predicted_Allele_AgeL2
    - MAF_Adj_LLD_AFRL2
    - Recomb_Rate_10kbL2
    - Nucleotide_Diversity_10kbL2
    - Backgrd_Selection_StatL2
    - CpG_Content_50kbL2
    - MAF_Adj_ASMCL2
    - GTEx_eQTL_MaxCPPL2
    - BLUEPRINT_H3K27acQTL_MaxCPPL2
    - BLUEPRINT_H3K4me1QTL_MaxCPPL2
    - BLUEPRINT_DNA_methylation_MaxCPPL2
    - synonymousL2
    - non_synonymousL2
    - Conserved_Vertebrate_phastCons46wayL2
    - Conserved_Mammal_phastCons46wayL2
    - Conserved_Primate_phastCons46wayL2
    - BivFlnkL2
    - Human_Promoter_VillarL2
    - Human_Enhancer_VillarL2
    - Ancient_Sequence_Age_Human_PromoterL2
    - Ancient_Sequence_Age_Human_EnhancerL2
    - Human_Enhancer_Villar_Species_Enhancer_CountL2
    - Human_Promoter_Villar_ExACL2
  ptb_model:
    nopooled: 
      - chip-seq-reproducible-ctr-H3K4me3
      - chip-seq-reproducible-dec-H3K27ac
      - chip-seq-reproducible-dec-H3K4me3
      - chip-seq-reproducible-dec-H3K4me1
      - chip-seq-reproducible-ctr-H3K4me1
      - atac-seq-reproducible-dec-ATAC
      - chip-seq-reproducible-ctr-H3K27ac
      - atac-seq-reproducible-ctr-ATAC
      - chip-seq-dec_up-H3K4me1
      - chip-seq-dec_up-H3K4me3
      - atac-seq-dec_down-ATAC
      - chip-seq-dec_up-H3K27ac
      - chip-seq-dec_down-H3K4me3
      - atac-seq-dec_up-ATAC
      - chip-seq-dec_down-H3K27ac
      - chip-seq-dec_down-H3K4me1
    full: 
      - chip-seq-pooled-DSC1-dec-H3K27ac
      - atac-seq-pooled-DSC2-dec-ATAC
      - chip-seq-pooled-DSC1-ctr-H3K4me3
      - atac-seq-pooled-DSC3-ctr-ATAC
      - chip-seq-pooled-DSC3-ctr-H3K4me1
      - chip-seq-pooled-DSC1-ctr-H3K4me1
      - atac-seq-pooled-DSC1-ctr-ATAC
      - chip-seq-pooled-DSC3-dec-H3K4me3
      - chip-seq-pooled-DSC2-ctr-H3K4me1
      - chip-seq-pooled-DSC1-dec-H3K4me1
      - chip-seq-pooled-DSC2-ctr-H3K27ac
      - chip-seq-pooled-DSC2-dec-H3K4me1
      - chip-seq-pooled-DSC1-dec-H3K4me3
      - chip-seq-pooled-DSC2-dec-H3K27ac
      - chip-seq-pooled-DSC3-dec-H3K27ac
      - chip-seq-pooled-DSC3-dec-H3K4me1
      - chip-seq-pooled-DSC2-dec-H3K4me3
      - chip-seq-pooled-DSC2-ctr-H3K4me3
      - atac-seq-pooled-DSC1-dec-ATAC
      - chip-seq-pooled-DSC1-ctr-H3K27ac
      - atac-seq-pooled-DSC2-ctr-ATAC
      - atac-seq-pooled-DSC3-dec-ATAC
      - chip-seq-pooled-DSC3-ctr-H3K27ac
      - chip-seq-reproducible-ctr-H3K4me3
      - chip-seq-reproducible-dec-H3K27ac
      - chip-seq-reproducible-dec-H3K4me3
      - chip-seq-reproducible-dec-H3K4me1
      - chip-seq-reproducible-ctr-H3K4me1
      - atac-seq-reproducible-dec-ATAC
      - chip-seq-reproducible-ctr-H3K27ac
      - atac-seq-reproducible-ctr-ATAC
      - chip-seq-dec_up-H3K4me1
      - chip-seq-dec_up-H3K4me3
      - chip-seq-pooled-DSC3-ctr-H3K4me3
      - atac-seq-dec_down-ATAC
      - chip-seq-dec_up-H3K27ac
      - chip-seq-dec_down-H3K4me3
      - atac-seq-dec_up-ATAC
      - chip-seq-dec_down-H3K27ac
      - chip-seq-dec_down-H3K4me1
    reproducible:
      - chip-seq-reproducible-ctr-H3K4me3
      - chip-seq-reproducible-dec-H3K4me3
      - chip-seq-reproducible-dec-H3K27ac
      - chip-seq-reproducible-ctr-H3K27ac
      - chip-seq-reproducible-dec-H3K4me1
      - chip-seq-reproducible-ctr-H3K4me1
      - atac-seq-reproducible-dec-ATAC
      - atac-seq-reproducible-ctr-ATAC
    reproducible_up_down:
      - chip-seq-reproducible-ctr-H3K4me3
      - chip-seq-reproducible-dec-H3K4me3
      - chip-seq-reproducible-dec-H3K27ac
      - chip-seq-reproducible-ctr-H3K27ac
      - chip-seq-reproducible-dec-H3K4me1
      - chip-seq-reproducible-ctr-H3K4me1
      - atac-seq-reproducible-dec-ATAC
      - atac-seq-reproducible-ctr-ATAC
      - chip-seq-dec_up-H3K4me1
      - chip-seq-dec_up-H3K4me3
      - atac-seq-dec_down-ATAC
      - chip-seq-dec_up-H3K27ac
      - chip-seq-dec_down-H3K4me3
      - atac-seq-dec_up-ATAC
      - chip-seq-dec_down-H3K27ac
      - chip-seq-dec_down-H3K4me1
      - hicd-seq-bait-dec-HIC
      - hicd-seq-target-dec-HIC
    reproducible_merged:
      - chip-seq-reproducible-ctr-H3K4me3
      - chip-seq-reproducible-dec-H3K4me3
      - chip-seq-reproducible-dec-H3K27ac
      - chip-seq-reproducible-ctr-H3K27ac
      - chip-seq-reproducible-dec-H3K4me1
      - chip-seq-reproducible-ctr-H3K4me1
      - atac-seq-reproducible-dec-ATAC
      - atac-seq-reproducible-ctr-ATAC
      - atac-seq-dec_diff-ATAC
      - chip-seq-dec_diff-H3K4me3
      - chip-seq-dec_diff-H3K27ac
      - chip-seq-dec_diff-H3K4me1
      - hicd-seq-both-dec-HIC

    



#+END_SRC

#+BEGIN_SRC snakemake :tangle '("../workflow/snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/snakefile" )


    import os
    import yaml
    from yaml import Loader
    from typing import Any,IO



    def host_loader(loader: yaml.loader.Loader,node: yaml.Node) -> Any:
          fields = loader.construct_mapping(node,deep=True)
          options=fields['options']
          # print([options[name] for name in options.keys()])
          ret_opt = [name for name in options.keys() if os.path.exists(options[name])]
          # print(ret_opt)
          return ret_opt[0]


    def dep_loader(loader: yaml.loader.Loader,node: yaml.Node) -> Any:
          options = loader.construct_mapping(node,deep=True)
          host = options['host']
          pref = options['pref']
          # print(pref)
          host =options['host']
          path = options['path']
          full_path = pref+path[host] if pref is not None else path[host]
          return full_path



    yaml.Loader.add_constructor('!Host', host_loader)
    yaml.Loader.add_constructor('!Dep', dep_loader)



    with open("../workflow/config_base.yaml") as stream:
          config=yaml.load(stream,Loader=Loader)

    config_d = config['paths']
    config_e = config['envs']
    shell.prefix(config_e["cmd_prefix"])
    # if config['flag_file']=="gardner":
    #       shell.prefix("source ~/.bashrc; ")
    # if config['flag_file']=="midway2":
    #       shell.prefix(". /scratch/midway2/nwknoblauch/spack/share/spack/setup-env.sh; ")


    with open("annots.yaml", 'r') as stream:
        all_annot = yaml.safe_load(stream)
        #(all_annot)
    wildcard_constraints:
        chrom="\d+",
        gwas="[fgdptb]+"

    localrules: all, get_hm3_snplist,get_plinkfiles,get_frq,get_weights


    include: "dl_snakefile"
    include: "gwas_snakefile"
    rule all:
        input:
             expand("{gwas}/{at}.results",gwas=["ptb","fgd"],at=["reproducible_up_down","reproducible_merged"])



#+END_SRC

** Downloading files

The first step is to download some LD score regression stuff from the web. In particular we want a gzipped tarball of the hapmap 3 SNPs.

#+BEGIN_SRC snakemake :tangle '("../workflow/dl_snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/dl_snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/dl_snakefile" )



  # rule get_hic:
  #     output:

  #     shell:
  #         "curl --digest --user {params.username}:{params.password} https://mnlab.uchicago.edu/mod/download/hi-c/DT1_dTL4_D_48h.ibed.bz2 --output {output}


  rule get_gest_dur_gwas:
      output:
          temp(config_d['GWAS']+"fetal_gest_duration/Fetal_gest_duration_NComms2019.txt.gz")
      shell:
          "wget http://mccarthy.well.ox.ac.uk/publications/2019/EggGestationalDuration_NatureCommunications/Fetal_gest_duration_NComms2019.txt.gz -O {output}"

  rule mv_fgd:
      input:
          config_d['GWAS']+"fetal_gest_duration/Fetal_gest_duration_NComms2019.txt.gz"
      output:
          temp(config_d['GWAS']+"input/fgd.txt")
      shell:
          "zcat {input} > {output}"


  rule mv_ptb:
      input:
          config_d['GWAS']+"meta.stat"
      output:
          temp(config_d['GWAS']+"input/ptb.txt")
      shell:
          "cp {input} {output}"        

  rule get_hm3_snplist:
      output:
          temp(config_d['DL'] +"hapmap3_snps.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/hapmap3_snps.tgz -O {output}"
#+END_SRC

Next we'll unzip the files and put them somewhere on disk.

#+BEGIN_SRC snakemake :tangle '("../workflow/dl_snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/dl_snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/dl_snakefile" )

  rule gunzip_hm3:
      input:
          rules.get_hm3_snplist.output
      params:
          dld=config_d['1KG']
      output:
          expand(config_d['1KG']+"hapmap3_snps/"+"hm.{chrom}.snp",chrom=range(1,23))
      shell:
          "tar -C {params.dld} -xvzf {input}"


#+END_SRC

** Preprocessing

*** rsid matching 

The rsids don't come with coordinates, and we don't have coordinates for our GWAS data, so we'll use the ~SNPlocs.Hsapiens.dbSNP144.GRCh37~ package 
to get the coordinates corresponding to these rsids.  Also note that we won't be able to get all of them, as some rsids have been merged by NCBI.

#+BEGIN_SRC R :tangle '("../scripts/rsid2loc.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/rsid2loc.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/rsid2loc.R" )

  library(dplyr)
library(purrr)
library(readr)


  library(ldmap)


  input_f <- snakemake@input[["input"]]
  output_f <- snakemake@output[["output"]]
  input_ids <- EigenH5::fast_str2int(scan(input_f, what = character()), prefix = "rs")
  input_ids <- input_ids[!is.na(input_ids)]
  BSgenome::snpsById(SNPlocs.Hsapiens.dbSNP144.GRCh37::SNPlocs.Hsapiens.dbSNP144.GRCh37,
                     ids = input_ids,
                     ifnotfound = "warn") %>% as_tibble() %>% 
      dplyr::rename(chrom = seqnames, rsid = RefSNP_id) %>%
      dplyr::mutate(chrom = as.integer(chrom),
                    rsid = rsid) %>%
      select(-strand) %>%
      readr::write_tsv(output_f)

#+END_SRC

#+RESULTS:

*** Annotation Merging

**** down+up->diff
We're going to merge the ~dec_down~ and ~dec_up~ annotations to create a ~dec_diff~ annotation

#+BEGIN_SRC R :tangle '("../scripts/merge_diff.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/merge_diff.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/merge_diff.R" )
  library(dplyr)
library(purrr)
library(readr)


  library(ldmap)
  library(EigenH5)

  input_down <- snakemake@input[["input_down"]]
  input_up <- snakemake@input[["input_up"]]

  outputf <- snakemake@output[["bedf"]]

  dcols <- cols(
    chrom = col_factor(paste0("chr", c(as.character(1:22), "X"))),
    start = col_integer(),
    end = col_integer())

  diff_df <- vroom::vroom(c(input_up, input_down),
                          delim = "\t",
                          col_names = c("chrom", "start", "end"),
                          col_types = dcols)
  new_ldmap_range(diff_df$chrom,
                  diff_df$start,
                  diff_df$end) %>%
    split_ldmap_range_overlap() %>%
    ldmap_range_2_data_frame() %>%
    vroom::vroom_write(outputf, delim = "\t", col_names = FALSE)
#+END_SRC




#+BEGIN_SRC snakemake :tangle '("../workflow/dl_snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/dl_snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/dl_snakefile" )

  rule merge_down_up:
      input:
          input_down=config_d['BED']+"{chip_atac}-seq-dec_down-{mark}.bed",
          input_up=config_d['BED']+"{chip_atac}-seq-dec_up-{mark}.bed"
      output:
          bedf=config_d['BED']+"{chip_atac}-seq-dec_diff-{mark}.bed"
      conda:
          config_e['r']
      script:
          "../scripts/merge_diff.R"

#+END_SRC

**** HiC combinations
I'll create three annotations out of the HiC data.  One will contain baits only, one targets only and one target|bait

#+BEGIN_SRC R :tangle '("../scripts/merge_hic.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/merge_hic.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/merge_hic.R" )
  library(dplyr)
  library(forcats)
library(purrr)
library(readr)


  library(ldmap)
  library(EigenH5)
  cold <- cols(
  bait_chr = col_factor(paste0("chr", c(as.character(1:22), c("X","Y")))),
  bait_start = col_double(),
  bait_end = col_double(),
  bait_name = col_character(),
  otherEnd_chr = col_factor(paste0("chr", c(as.character(1:22), c("X","Y")))),
  otherEnd_start = col_double(),
  otherEnd_end = col_double(),
  otherEnd_name = col_character(),
  N_reads = col_double(),
  score = col_double()
  )
  input_hic <- read_tsv(snakemake@input[["inputf"]],col_names=names(cold$cols),col_types=cold,skip=1L) %>%
    filter(bait_chr!="chrY", otherEnd_chr!="chrY")  %>%
    mutate(bait_chr = fct_drop(bait_chr), otherEnd_chr = fct_drop(otherEnd_chr))

  baitf <- snakemake@output[["bait"]]
  targetf <- snakemake@output[["target"]]
  bothf <- snakemake@output[["both"]]

  bait_ld <- new_ldmap_range(input_hic$bait_chr,
                             input_hic$bait_start,
                             input_hic$bait_end)

  target_ld <- new_ldmap_range(input_hic$otherEnd_chr,
                               input_hic$otherEnd_start,
                               input_hic$otherEnd_end)

  both_ld <- merge_ldmap_ranges(bait_ld,target_ld)

  ldmap_range_2_data_frame(bait_ld) %>% write_tsv(baitf,col_names = FALSE)
  ldmap_range_2_data_frame(target_ld) %>% write_tsv(targetf,col_names = FALSE)
  ldmap_range_2_data_frame(both_ld) %>% write_tsv(bothf,col_names = FALSE)


#+END_SRC

#+RESULTS:



#+BEGIN_SRC snakemake :tangle '("../workflow/dl_snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/dl_snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/dl_snakefile" )

  rule merge_split_hic:
      input:
          inputf=config_d['BED']+"DT1_dTL4_D_48h.ibed.bz2",
      output:
          bait=config_d['BED']+"hicd-seq-bait-dec-HIC.bed",
          target=config_d['BED']+"hicd-seq-target-dec-HIC.bed",
          both=config_d['BED']+"hicd-seq-both-dec-HIC.bed"
      conda:
          config_e['r']
      script:
          "../scripts/merge_hic.R"

#+END_SRC





** Munging the GWAS data

Unfortunately I don't have a remote source for the gwas summary statistics I can point you to, so we'll just pretend like you know
how to get to `meta.stat` the PTB gwas file.  First thing is to convert it to HDF5 for easier read/write of subsets

*** Munging strategy
We're going to create a ~cols~ object for each file. We'll ignore column names in every instance and use our own. if there are no column headers, we'll make a ~params~ argument


#+BEGIN_SRC R :tangle '("../scripts/ptbcols.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/ptbcols.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/ptbcols.R" )
  mc <- cols(
      rsid = col_character(),
      chrom = col_integer(),
      pos = col_double(),
      A1 = col_character(),
      A2 = col_character(),
      N = col_double(),
      freq = col_double(),
      beta = col_double(),
      se = col_double(),
      pval = col_double(),
      Q = col_double(),
      het = col_double(),
      N.local = col_double(),
      freq.local = col_double(),
      beta.local = col_double(),
      se.local = col_double(),
      pval.local = col_double(),
      N.23andMe = col_double(),
      freq.23andMe = col_double(),
      beta.23andMe = col_double(),
      se.23andMe = col_double(),
      pval.23andMe = col_double()
  )
data_delim <- "\t"

#+END_SRC

#+BEGIN_SRC R :tangle '( "../scripts/fgdcols.R" "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/fgdcols.R" )

mc <- cols(
  chrom = col_integer(), # Chr
  pos = col_double(), #Pos
  rsid = col_character(), #Rsid
  A1 = col_character(), #Effect_allele
  A2 = col_character(), #Non_effect_allele
  beta = col_double(), #Effect
  se = col_double(), #StdErr
  pval = col_double(), #P
  HetPVal = col_double(),
  N = col_double(),
  SNP = col_character()
)
data_delim <- " "

#+END_SRC




#+BEGIN_SRC R :tangle '("../scripts/gwas2h5.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/gwas2h5.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/gwas2h5.R" )

  library(dplyr)
library(purrr)
library(readr)


  library(EigenH5)
  library(readr)
  library(ldmap)


  ## mc <- cols(
  ##     rsid = col_character(),
  ##     chrom = col_integer(),
  ##     pos = col_double(),
  ##     A1 = col_character(),
  ##     A2 = col_character(),
  ##     N = col_double(),
  ##     freq = col_double(),
  ##     beta = col_double(),
  ##     se = col_double(),
  ##     pval = col_double(),
  ##     Q = col_double(),
  ##     het = col_double(),
  ##     N.local = col_double(),
  ##     freq.local = col_double(),
  ##     beta.local = col_double(),
  ##     se.local = col_double(),
  ##     pval.local = col_double(),
  ##     N.23andMe = col_double(),
  ##     freq.23andMe = col_double(),
  ##     beta.23andMe = col_double(),
  ##     se.23andMe = col_double(),
  ##     pval.23andMe = col_double()
  ## )


  input_f <- snakemake@input[["inputf"]]
  output_f <- snakemake@output[["outputf"]]
  paramf <- snakemake@input[["paramf"]]
  stopifnot(!is.null(paramf))
  source(paramf)


  callback_fun <- function(df, filename, datapath, ...){
    write_df_h5(
      df = dplyr::slice(
                    dplyr::mutate(df,
                                  ref = fast_str2ascii(A2),
                                  alt = fast_str2ascii(A1),
                                  snp_struct =
                                    new_ldmap_snp(chrom, pos, ref, alt),
                                  rsid = fast_str2int(rsid, prefix = "rs"),
                                  ),
                    rank.ldmap_snp(snp_struct)),
      filename = filename, datapath = datapath, ... = ...)
  }

  stopifnot(!is.null(input_f),
            !is.null(output_f),
            file.exists(input_f),
            !file.exists(output_f))

  delim2h5(input_f,
           output_file = output_f,
           h5_args = list(datapath = "snp"),
           delim = data_delim,
           col_names = names(mc$cols),
           skip = 1L,
           callback_fun = callback_fun,
           col_types = mc,
           progress = TRUE,
           chunk_size = 150000)

  chrom_vec <- read_vector_h5v(output_f, "snp/chrom", i = integer())
  chrom_df <- rle2offset(chrom_vec) %>%
      dplyr::rename(chrom = value)
  write_df_h5(chrom_df,output_f,"chrom_offset")
#+END_SRC




#+BEGIN_SRC snakemake :tangle '("../workflow/gwas_snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/gwas_snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/gwas_snakefile" )

  rule ptb_gwas2h5:
      input:
          inputf=config_d['GWAS']+"input/{gwas}.txt",
          paramf="../scripts/{gwas}cols.R"
      output:
          outputf=protected(config_d['GWAS'] +"{gwas}_gwas.h5")
      conda:
          config_e['r']
      script:
          "../scripts/gwas2h5.R"



#+END_SRC



Next is to write some code to pull out the indices with the matching rsids (using coordinates, not rsid)


#+BEGIN_SRC R :tangle '("../scripts/index_gwas.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/index_gwas.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/index_gwas.R" )

  library(dplyr)
library(purrr)
library(readr)


  library(EigenH5)
  library(readr)
  library(ldmap)
  ## setwd("~/Dropbox/Repos/ldsc/workflow/")
  ##   load("~/Dropbox/Repos/ldsc/workflow/tf.RData")

    input_f <- snakemake@input[["inputf"]]
    index_f <-  snakemake@input[["indexf"]]
    chrom <- snakemake@params[["chrom"]]
    stopifnot(!is.null(chrom))
    schrom <- as.integer(chrom)
    output_f <- snakemake@output[["outputf"]]


    ind_spec <- cols_only(
      CHR = col_integer(),
      BP = col_double(),
      SNP = col_character()
    )

    gwas_type <- if_else(
      is.null(snakemake@params[["gwas_t"]]),
      "",
      paste0(".", snakemake@params[["gwas_t"]])
    )


    beta_col <- glue::glue("beta{gwas_type}")
    se_col <- glue::glue("se{gwas_type}")
    N_col <- glue::glue("N{gwas_type}")
    P_col <- glue::glue("pval{gwas_type}")

    sel_cols <- c("snp_struct",
                  beta_col,
                  "A1",
                  "A2",
                  se_col,
                  N_col,
                  P_col)

    sel_cols <- stringr::str_replace(
                           sel_cols,
                           "\\.$",
                           "")

    index_df <- vroom::vroom(
                         index_f,
                         delim = "\t",
                         col_types = ind_spec
                       )  %>% 
      rename(chrom=CHR,rsid=SNP,pos=BP)
      nr_index_df <- nrow(index_df)

    chrom_df <- read_tibble_h5(input_f, "chrom_offset", list()) %>%
      filter(chrom == schrom) %>% mutate(offset = as.integer(offset), datasize = as.integer(datasize)) %>% 
      arrange(offset)

    jdf <- pmap_dfr(chrom_df, function(chrom, datasize, offset) {
  #    subset_l <- seq(offset + 1, length.out = datasize)
      input_i <- EigenH5::read_df_h5(filename = input_f,
                              datapath = "snp",
                                subcols = sel_cols,
                                offset=offset,
                                datasize=datasize) %>%
        mutate(subset = (1:n()) + offset)

        inner_join(index_df,  bind_cols(input_i,ldmap::ldmap_snp_2_dataframe(input_i$snp_struct)))
    })

                                          #%>% mutate(snp_struct = as_ldmap_snp(snp_struct))  %>%
  stopifnot(all(jdf$chrom == schrom))
  stopifnot(nrow(jdf)>0)
  ## stopifnot(nrow(jdf) == nr_index_df)

    jdf  %>% rename(beta =  {{beta_col}},
                    se =  {{se_col}},
                    N =  {{N_col}}) %>%
      dplyr::distinct(rsid, .keep_all = TRUE) %>% 
      dplyr::transmute(SNP = rsid, N = N, Z = beta / se, A1 = A1, A2 = A2,P=pval) %>%
      vroom::vroom_write(output_f,delim = "\t")
#+END_SRC

#+BEGIN_SRC R :tangle '("../scripts/gen_ldsc_sumstats.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/gen_ldsc_sumstats.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/gen_ldsc_sumstats.R" )
library(vroom)
library(magrittr)

 input_f <- snakemake@input[["inputf"]]
 output <- snakemake@output[["outputf"]]

 vroom::vroom(input_f,delim="\t") %>% vroom_write(output,delim="\t")


#+END_SRC


#+BEGIN_SRC snakemake :tangle '("../workflow/gwas_snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/gwas_snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/gwas_snakefile" )


  rule indexgwas2h5:
      input:
          inputf=config_d['GWAS'] +"{gwas}_gwas.h5",
          indexf=config_d['L2'] +"baseline/baselineLD.{chrom}.l2.ldscore.gz"
      params:
          chrom="{chrom}"
      output:
          outputf=temp(config_d['GWAS'] +"hm3_index/{gwas}_gwas_hm_chr{chrom}.tsv")
      conda:
          config_e['r']
      script:
          "../scripts/index_gwas.R"

  rule prep_ldsc_sumstsat:
      input:
          inputf=expand(config_d['GWAS'] +"hm3_index/{{gwas}}_gwas_hm_chr{chrom}.tsv",chrom=range(1,23))
      params:
          gwas_t=""
      output:
          outputf=temp(config_d['GWAS'] +"ldsc_input_pre/{gwas}_gwas.sumstats.gz")
      conda:
          config_e['r']
      script:
          "../scripts/gen_ldsc_sumstats.R"


  rule check_ldsc_sumstat:
      input:
          config_d['GWAS'] +"ldsc_input_pre/{gwas}_gwas.sumstats.gz"
      output:
          outputf=config_d['GWAS'] +"ldsc_input/{gwas}_gwas.sumstats.gz"
      params:
          outputf=config_d['GWAS'] +"ldsc_input/{gwas}_gwas"
      conda:
          config_e['ldsc']
      log:
          logf=config_d['GWAS'] +"ldsc_input/{gwas}_gwas.log"
      shell:
          "munge_sumstats.py --sumstats {input} --out {params.outputf}"
#+END_SRC

#+BEGIN_SRC bash :session rcc2 :dir /ssh:rcc2:/project2/xinhe/software/ldsc/workflow/
. "/project2/xinhe/software/miniconda3/etc/profile.d/conda.sh"
conda activate cause_gwas
snakemake -n


#+END_SRC

** Running LDSC

#+BEGIN_SRC snakemake :tangle '("../workflow/dl_snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/dl_snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/dl_snakefile" )

  rule get_cadd:
      output:
          temp(config_d["DL"]+"whole_genome_SNVs_inclAnno.tsv.gz")
      shell:
          "wget https://krishna.gs.washington.edu/download/CADD/v1.4/GRCh37/whole_genome_SNVs_inclAnno.tsv.gz -O {output}"

  rule get_spidex:
      output:
          temp(config_d["DL"]+"hg19_spidex.zip")
      shell:
          "wget http://www.openbioinformatics.org/annovar/download/IlvUMvrpPT/hg19_spidex.zip -O {output}"
  rule get_baseline_model:
      output:
          temp(config_d['DL']+"1000G_Phase3_baselineLD_v2.2_ldscores.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_baselineLD_v2.2_ldscores.tgz -O {output}"

  rule get_weights:
      output:
          temp(config_d["DL"]+"1000G_Phase3_weights_hm3_no_MHC.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_weights_hm3_no_MHC.tgz -O {output}"

  rule gunzip_weights:
      input:
          config_d["DL"]+"1000G_Phase3_weights_hm3_no_MHC.tgz"
      output:
          ldfiles = expand(config_d['WEIGHTS'] +"weights.hm3_noMHC.{chrom}.l2.ldscore.gz",chrom=range(1,23))
      params:
          W=config_d['1KG']
      shell:
          "tar -xvzf {input} -C {params.W}"        

  rule get_frq:
      output:
          temp(config_d['DL']+"1000G_Phase3_frq.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_frq.tgz -O {output}"


  rule get_plinkfiles:
      output:
          temp(config_d['DL'] +"1000G_Phase3_plinkfiles.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_plinkfiles.tgz -O {output}"

  rule gunzip_plinkfiles:
      input:
          config_d['DL'] +"1000G_Phase3_plinkfiles.tgz"
      output:
          fam_files = expand(config_d['1KG'] +"1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.fam",chrom=range(1,23)),
          bim_files = expand(config_d['1KG'] +"1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bim",chrom=range(1,23)),
          bed_files = expand(config_d['1KG'] +"1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bed",chrom=range(1,23))
      params:
          KG=config_d['1KG']
      shell:
          "tar -xvzf {input} -C {params.KG}"

  rule gunzip_frqf:
      input:
          config_d['DL'] +"1000G_Phase3_frq.tgz"
      output:
          fam_files = expand(config_d['FRQF'] +"1000G.EUR.QC.{chrom}.frq",chrom=range(1,23)),
      params:
          KG=config_d['1KG']
      shell:
          "tar -xvzf {input} -C {params.KG}"


  rule gunzip_baseline:
      input:
          config_d['DL'] +"1000G_Phase3_baselineLD_v2.2_ldscores.tgz"
      output:
          ldfiles = expand(config_d['L2'] +"baseline/baselineLD.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
          annotf = expand(config_d['L2'] +"baseline/baselineLD.{chrom}.annot.gz",chrom=range(1,23)),
          m50 = expand(config_d['L2'] +"baseline/baselineLD.{chrom}.l2.M_5_50",chrom=range(1,23))
      params:
          L2=config_d['L2']
      shell:
          "tar -xvzf {input} -C {params.L2}/baseline"



  rule unzip_annot:
      input:
          config_d['BED'] + "{annot}.bed.bz2"
      output:
          temp(config_d['BED'] + "{annot}.bed")
      wildcard_constraints:
          annot="[^/]+"
      shell:
          "bzip2 -cd {input} > {output}"


  rule make_annot:
      input:
          anno_bed=config_d['BED'] +"{annot}.bed",
          bim=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bim"
      output:
          annot = config_d['L2'] +"{annot}.{chrom}.annot.gz"
      params:
          anno_name='{annot}'
      conda:
          config_e['ldsc']
      shell:
          "make_annot.py --bed-file {input.anno_bed} --bimfile {input.bim} --annot-file {output.annot} --annot-name {params.anno_name}"

  rule pull_rsid:
      input:
          config_d["L2"]+"baseline/baselineLD.{chrom}.l2.ldscore.gz"
      output:
          temp(config_d["L2"]+"snplist/{chrom}.snplist.txt")
      shell:
          "zcat {input} | cut -f 2 | tail -n +2 > {output}"



  rule cmp_ldscores:
      input:
          anno_bed=config_d['L2'] +"{annot}.{chrom}.annot.gz",
          snplistf=config_d["L2"]+"snplist/{chrom}.snplist.txt",
          bim=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bim",
          bed=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bed",
          fam=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.fam"
      output:
          l2=config_d['L2']+"{annot}.{chrom}.l2.M",
          l2M_50=config_d['L2']+"{annot}.{chrom}.l2.M_5_50",
          l2gz=config_d['L2']+"{annot}.{chrom}.l2.ldscore.gz"
      params:
          plink=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}",
          odir=config_d['L2']+"{annot}.{chrom}",
          anno="{annot}"
      # wildcard_constraints:
      #     annot="[^/]"
      conda:
          config_e['ldsc']
      shell:
          "ldsc.py --l2 --bfile {params.plink} --print-snps {input.snplistf} --ld-wind-cm 1 --thin-annot --annot {input.anno_bed} --out {params.odir} && cp {output.l2gz} {output.l2gz}~ && zcat {output.l2gz}~ | sed '1s/L2/{params.anno}/' | gzip  > {output.l2gz} && rm {output.l2gz}~"


#+END_SRC

#+BEGIN_SRC R :tangle '("../scripts/check_ldscfiles.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/check_ldscfiles.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/check_ldscfiles.R" )

  library(vroom)
  library(dplyr)
library(purrr)
library(readr)


  library(fs)

  ## save.image("wsl.RDS")
  ## stop()
  ## setwd("~/Dropbox/Repos/ldsc/workflow")
  ## load("wsl.RDS")

  ## yam
  ## l_file <- yaml::yaml.load_file("../workflow/annots.yaml")

  feat_list <- snakemake@params[["features"]]
  baseline_feat <- snakemake@params[["baseline_features"]]
  annot_name <- snakemake@params[["annot_name"]]
  l2dir <- snakemake@params[["L2"]]

  stopifnot(!is.null(feat_list),!is.null(baseline_feat),!is.null(annot_name),!is.null(l2dir))

#+END_SRC

#+BEGIN_SRC R :tangle '("../scripts/check_ldscfiles.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/check_ldscfiles.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/check_ldscfiles.R" )
   file_df <- tibble::as_tibble(expand.grid(feature = feat_list, chrom = 1:22,stringsAsFactors = FALSE)) %>%
    dplyr::mutate(
             path = fs::path(l2dir,
                             paste0( feature, ".", chrom, ".l2.ldscore.gz")),
             annot_path = fs::path(l2dir,
                             paste0( feature, ".", chrom, ".annot.gz")),
             baseline_path = fs::path(l2dir,"baseline/", paste0("baselineLD.", chrom, ".l2.ldscore.gz")),
             baseline_annot_path = fs::path(l2dir,"baseline/", paste0("baselineLD.", chrom, ".annot.gz")),
             new_path = fs::path(l2dir,"new_baseline/", paste0(annot_name,".", chrom, ".l2.ldscore.gz")),
             new_annot_path = fs::path(l2dir,"new_baseline/", paste0(annot_name,".", chrom, "annot.gz"))
                 )





  stopifnot(all(fs::file_exists(c(file_df$path,file_df$baseline_path,file_df$annot_path,file_df$baseline_annot_path))))
#+END_SRC

#+BEGIN_SRC R :tangle '("../scripts/check_ldscfiles.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/check_ldscfiles.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/check_ldscfiles.R" )
  spec <- cols(
    CHR = col_skip(),
    SNP = col_skip(),
    BP = col_skip(),
    L2 = col_double()
  )
  spec_base <- cols(
    CHR = col_double(),
    SNP = col_character(),
    BP = col_double(),
    baseL2 = col_double(),
    Coding_UCSCL2 = col_double(),
    Coding_UCSC.flanking.500L2 = col_double(),
    Conserved_LindbladTohL2 = col_double(),
    Conserved_LindbladToh.flanking.500L2 = col_double(),
    CTCF_HoffmanL2 = col_double(),
    CTCF_Hoffman.flanking.500L2 = col_double(),
    DGF_ENCODEL2 = col_double(),
    DGF_ENCODE.flanking.500L2 = col_double(),
    DHS_peaks_TrynkaL2 = col_double(),
    DHS_TrynkaL2 = col_double(),
    DHS_Trynka.flanking.500L2 = col_double(),
    Enhancer_AnderssonL2 = col_double(),
    Enhancer_Andersson.flanking.500L2 = col_double(),
    Enhancer_HoffmanL2 = col_double(),
    Enhancer_Hoffman.flanking.500L2 = col_double(),
    FetalDHS_TrynkaL2 = col_double(),
    FetalDHS_Trynka.flanking.500L2 = col_double(),
    H3K27ac_HniszL2 = col_double(),
    H3K27ac_Hnisz.flanking.500L2 = col_double(),
    H3K27ac_PGC2L2 = col_double(),
    H3K27ac_PGC2.flanking.500L2 = col_double(),
    H3K4me1_peaks_TrynkaL2 = col_double(),
    H3K4me1_TrynkaL2 = col_double(),
    H3K4me1_Trynka.flanking.500L2 = col_double(),
    H3K4me3_peaks_TrynkaL2 = col_double(),
    H3K4me3_TrynkaL2 = col_double(),
    H3K4me3_Trynka.flanking.500L2 = col_double(),
    H3K9ac_peaks_TrynkaL2 = col_double(),
    H3K9ac_TrynkaL2 = col_double(),
    H3K9ac_Trynka.flanking.500L2 = col_double(),
    Intron_UCSCL2 = col_double(),
    Intron_UCSC.flanking.500L2 = col_double(),
    PromoterFlanking_HoffmanL2 = col_double(),
    PromoterFlanking_Hoffman.flanking.500L2 = col_double(),
    Promoter_UCSCL2 = col_double(),
    Promoter_UCSC.flanking.500L2 = col_double(),
    Repressed_HoffmanL2 = col_double(),
    Repressed_Hoffman.flanking.500L2 = col_double(),
    SuperEnhancer_HniszL2 = col_double(),
    SuperEnhancer_Hnisz.flanking.500L2 = col_double(),
    TFBS_ENCODEL2 = col_double(),
    TFBS_ENCODE.flanking.500L2 = col_double(),
    Transcr_HoffmanL2 = col_double(),
    Transcr_Hoffman.flanking.500L2 = col_double(),
    TSS_HoffmanL2 = col_double(),
    TSS_Hoffman.flanking.500L2 = col_double(),
    UTR_3_UCSCL2 = col_double(),
    UTR_3_UCSC.flanking.500L2 = col_double(),
    UTR_5_UCSCL2 = col_double(),
    UTR_5_UCSC.flanking.500L2 = col_double(),
    WeakEnhancer_HoffmanL2 = col_double(),
    WeakEnhancer_Hoffman.flanking.500L2 = col_double(),
    GERP.NSL2 = col_double(),
    GERP.RSsup4L2 = col_double(),
    MAFbin1L2 = col_double(),
    MAFbin2L2 = col_double(),
    MAFbin3L2 = col_double(),
    MAFbin4L2 = col_double(),
    MAFbin5L2 = col_double(),
    MAFbin6L2 = col_double(),
    MAFbin7L2 = col_double(),
    MAFbin8L2 = col_double(),
    MAFbin9L2 = col_double(),
    MAFbin10L2 = col_double(),
    MAF_Adj_Predicted_Allele_AgeL2 = col_double(),
    MAF_Adj_LLD_AFRL2 = col_double(),
    Recomb_Rate_10kbL2 = col_double(),
    Nucleotide_Diversity_10kbL2 = col_double(),
    Backgrd_Selection_StatL2 = col_double(),
    CpG_Content_50kbL2 = col_double(),
    MAF_Adj_ASMCL2 = col_double(),
    GTEx_eQTL_MaxCPPL2 = col_double(),
    BLUEPRINT_H3K27acQTL_MaxCPPL2 = col_double(),
    BLUEPRINT_H3K4me1QTL_MaxCPPL2 = col_double(),
    BLUEPRINT_DNA_methylation_MaxCPPL2 = col_double(),
    synonymousL2 = col_double(),
    non_synonymousL2 = col_double(),
    Conserved_Vertebrate_phastCons46wayL2 = col_double(),
    Conserved_Vertebrate_phastCons46way.flanking.500L2 = col_double(),
    Conserved_Mammal_phastCons46wayL2 = col_double(),
    Conserved_Mammal_phastCons46way.flanking.500L2 = col_double(),
    Conserved_Primate_phastCons46wayL2 = col_double(),
    Conserved_Primate_phastCons46way.flanking.500L2 = col_double(),
    BivFlnkL2 = col_double(),
    BivFlnk.flanking.500L2 = col_double(),
    Human_Promoter_VillarL2 = col_double(),
    Human_Promoter_Villar.flanking.500L2 = col_double(),
    Human_Enhancer_VillarL2 = col_double(),
    Human_Enhancer_Villar.flanking.500L2 = col_double(),
    Ancient_Sequence_Age_Human_PromoterL2 = col_double(),
    Ancient_Sequence_Age_Human_Promoter.flanking.500L2 = col_double(),
    Ancient_Sequence_Age_Human_EnhancerL2 = col_double(),
    Ancient_Sequence_Age_Human_Enhancer.flanking.500L2 = col_double(),
    Human_Enhancer_Villar_Species_Enhancer_CountL2 = col_double(),
    Human_Promoter_Villar_ExACL2 = col_double(),
    Human_Promoter_Villar_ExAC.flanking.500L2 = col_double()
  )



  anno_cols <- cols(
    CHR = col_double(),
    BP = col_double(),
    SNP = col_character(),
    CM = col_double(),
    base = col_double(),
    Coding_UCSC = col_double(),
    Coding_UCSC.flanking.500 = col_double(),
    Conserved_LindbladToh = col_double(),
    Conserved_LindbladToh.flanking.500 = col_double(),
    CTCF_Hoffman = col_double(),
    CTCF_Hoffman.flanking.500 = col_double(),
    DGF_ENCODE = col_double(),
    DGF_ENCODE.flanking.500 = col_double(),
    DHS_peaks_Trynka = col_double(),
    DHS_Trynka = col_double(),
    DHS_Trynka.flanking.500 = col_double(),
    Enhancer_Andersson = col_double(),
    Enhancer_Andersson.flanking.500 = col_double(),
    Enhancer_Hoffman = col_double(),
    Enhancer_Hoffman.flanking.500 = col_double(),
    FetalDHS_Trynka = col_double(),
    FetalDHS_Trynka.flanking.500 = col_double(),
    H3K27ac_Hnisz = col_double(),
    H3K27ac_Hnisz.flanking.500 = col_double(),
    H3K27ac_PGC2 = col_double(),
    H3K27ac_PGC2.flanking.500 = col_double(),
    H3K4me1_peaks_Trynka = col_double(),
    H3K4me1_Trynka = col_double(),
    H3K4me1_Trynka.flanking.500 = col_double(),
    H3K4me3_peaks_Trynka = col_double(),
    H3K4me3_Trynka = col_double(),
    H3K4me3_Trynka.flanking.500 = col_double(),
    H3K9ac_peaks_Trynka = col_double(),
    H3K9ac_Trynka = col_double(),
    H3K9ac_Trynka.flanking.500 = col_double(),
    Intron_UCSC = col_double(),
    Intron_UCSC.flanking.500 = col_double(),
    PromoterFlanking_Hoffman = col_double(),
    PromoterFlanking_Hoffman.flanking.500 = col_double(),
    Promoter_UCSC = col_double(),
    Promoter_UCSC.flanking.500 = col_double(),
    Repressed_Hoffman = col_double(),
    Repressed_Hoffman.flanking.500 = col_double(),
    SuperEnhancer_Hnisz = col_double(),
    SuperEnhancer_Hnisz.flanking.500 = col_double(),
    TFBS_ENCODE = col_double(),
    TFBS_ENCODE.flanking.500 = col_double(),
    Transcr_Hoffman = col_double(),
    Transcr_Hoffman.flanking.500 = col_double(),
    TSS_Hoffman = col_double(),
    TSS_Hoffman.flanking.500 = col_double(),
    UTR_3_UCSC = col_double(),
    UTR_3_UCSC.flanking.500 = col_double(),
    UTR_5_UCSC = col_double(),
    UTR_5_UCSC.flanking.500 = col_double(),
    WeakEnhancer_Hoffman = col_double(),
    WeakEnhancer_Hoffman.flanking.500 = col_double(),
    GERP.NS = col_double(),
    GERP.RSsup4 = col_double(),
    MAFbin1 = col_double(),
    MAFbin2 = col_double(),
    MAFbin3 = col_double(),
    MAFbin4 = col_double(),
    MAFbin5 = col_double(),
    MAFbin6 = col_double(),
    MAFbin7 = col_double(),
    MAFbin8 = col_double(),
    MAFbin9 = col_double(),
    MAFbin10 = col_double(),
    MAF_Adj_Predicted_Allele_Age = col_double(),
    MAF_Adj_LLD_AFR = col_double(),
    Recomb_Rate_10kb = col_double(),
    Nucleotide_Diversity_10kb = col_double(),
    Backgrd_Selection_Stat = col_double(),
    CpG_Content_50kb = col_double(),
    MAF_Adj_ASMC = col_double(),
    GTEx_eQTL_MaxCPP = col_double(),
    BLUEPRINT_H3K27acQTL_MaxCPP = col_double(),
    BLUEPRINT_H3K4me1QTL_MaxCPP = col_double(),
    BLUEPRINT_DNA_methylation_MaxCPP = col_double(),
    synonymous = col_double(),
    non_synonymous = col_double(),
    Conserved_Vertebrate_phastCons46way = col_double(),
    Conserved_Vertebrate_phastCons46way.flanking.500 = col_double(),
    Conserved_Mammal_phastCons46way = col_double(),
    Conserved_Mammal_phastCons46way.flanking.500 = col_double(),
    Conserved_Primate_phastCons46way = col_double(),
    Conserved_Primate_phastCons46way.flanking.500 = col_double(),
    BivFlnk = col_double(),
    BivFlnk.flanking.500 = col_double(),
    Human_Promoter_Villar = col_double(),
    Human_Promoter_Villar.flanking.500 = col_double(),
    Human_Enhancer_Villar = col_double(),
    Human_Enhancer_Villar.flanking.500 = col_double(),
    Ancient_Sequence_Age_Human_Promoter = col_double(),
    Ancient_Sequence_Age_Human_Promoter.flanking.500 = col_double(),
    Ancient_Sequence_Age_Human_Enhancer = col_double(),
    Ancient_Sequence_Age_Human_Enhancer.flanking.500 = col_double(),
    Human_Enhancer_Villar_Species_Enhancer_Count = col_double(),
    Human_Promoter_Villar_ExAC = col_double(),
    Human_Promoter_Villar_ExAC.flanking.500 = col_double()
  )
#+END_SRC

#+BEGIN_SRC R :tangle '("../scripts/check_ldscfiles.R" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../scripts/check_ldscfiles.R"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../scripts/check_ldscfiles.R" )

  file_df <- nest(file_df,feat_data = c(feature,path,annot_path))
  modify_cols <- function(cols,old,new) {
    cols$cols <- set_names(cols$cols, ~dplyr::if_else(.== old,new, .))
    return(cols)
  }


  feat_fun <- function(f,feat) {
    return(tibble::tibble(!!feat := scan(f,what = numeric(),skip = 1)))
    }


  pwalk(file_df,function(baseline_path, chrom, feat_data, new_path, new_annot_path,baseline_annot_path) {
    cat("Now on",chrom,"\n")
    keep_cols <- c("CHR","SNP","BP","baseL2",baseline_feat)

    bldcols <- names(spec_base$cols)
    bacols <- names(anno_cols$cols)
    stopifnot(all(keep_cols %in%  names(spec_base$cols)))
    stopifnot(all(keep_cols %in%  names(anno_cols$cols)))
    bad_good_cols <- keep_cols[!keep_cols %in%  names(anno_cols$cols)]
  
    bad_cols <- names(spec_base$cols)[!(names(spec_base$cols) %in% keep_cols)]
    bad_anno_cols <- str_replace(bad_cols,"L2$","")

    new_base <- spec_base
    new_anno_spec <- anno_cols
    for (bc in seq_along(bad_cols)) {
      new_base$cols[[bad_cols[bc]]] <- col_skip()
      new_anno_spec$cols[[bad_anno_cols[bc]]] <- col_skip()
    }
  
    feat_data <- mutate(feat_data,col_spec = map(feature, ~modify_cols(spec, "L2" , .x)))
    annot_df <- map2_dfc(feat_data$annot_path,feat_data$feature, ~feat_fun(.x, .y))
    o_anno_path <- bind_cols(vroom::vroom(baseline_annot_path,col_types = new_anno_spec,delim = "\t"),annot_df)
    tannot <- read_delim(feat_data$annot_path[1],delim = "\t")
    lddf <- pmap_dfc(feat_data,function(feature,path,col_spec, ...) {
      vroom::vroom(path,col_names = names(col_spec$cols),col_types = col_spec,delim = "\t",skip = 1L)})
    baseline_df <- bind_cols(vroom::vroom(baseline_path,delim = "\t",col_types = new_base),lddf)
    vroom::vroom_write(baseline_df,new_path,delim = "\t")
  })




om(baseline_path,delim = "\t",col_types = new_base),lddf)
    vroom::vroom_write(baseline_df,new_path,delim = "\t")
  })


#+END_SRC




#+BEGIN_SRC snakemake :tangle '("../workflow/dl_snakefile" "/ssh:gardner:/gpfs/data/xhe-lab/software/ldsc/docs/../workflow/dl_snakefile"  "/ssh:rcc2:/project2/xinhe/software/ldsc/docs/../workflow/dl_snakefile" )




    def get_annot_files(wildcards):
        return {'anno_l2':expand(config_d['L2'] +"{anno_name}.{chrom}.l2.ldscore.gz",chrom=range(1,23),anno_name=all_annot['ptb_model'][wildcards.anno_name]),
              'baseline_l2':expand(config_d['L2'] +"baseline/baselineLD.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
              'gwasf':config_d['GWAS'] +f"ldsc_input/{wildcards.gwas}_gwas.sumstats.gz",
              'baselinef':  expand(config_d['WEIGHTS'] +"weights.hm3_noMHC.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
              'freqf':  expand(config_d['FRQF'] +"1000G.EUR.QC.{chrom}.frq",chrom=range(1,23)),
      }


  def get_annot_pairs(wildcards):
      return {'anno_l2':expand(config_d['L2'] +"{anno_name}.{chrom}.l2.ldscore.gz",chrom=range(1,23),anno_name=all_annot['ptb_model'][wildcards.anno_name]),
              'baseline_l2':expand(config_d['L2'] +"baseline/baselineLD.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
              'gwasfA':config_d['GWAS'] +f"ldsc_input/{wildcards.gwasA}_gwas.sumstats.gz",
              'gwasfB':config_d['GWAS'] +f"ldsc_input/{wildcards.gwasB}_gwas.sumstats.gz",
              'baselinef':  expand(config_d['WEIGHTS'] +"weights.hm3_noMHC.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
              'freqf':  expand(config_d['FRQF'] +"1000G.EUR.QC.{chrom}.frq",chrom=range(1,23)),
      }


  rule run_ldsc:
      input:
          unpack(get_annot_files)
      output:
          dataf="{gwas}/{anno_name}.results"
      log:
          tempf=temp("{gwas}_{anno_name}.log")
      params:
          annot=lambda wildcards: ','.join(expand(config_d['L2']+"{anno_name}.",anno_name=all_annot['ptb_model'][wildcards.anno_name])),
          baseline=config_d['L2']+"baseline/baselineLD.",
          weights=config_d['WEIGHTS']+"weights.hm3_noMHC.",
          frq=config_d['FRQF'] +"1000G.EUR.QC.",
          odir="{gwas}/{anno_name}"
      conda:
          config_e['ldsc']
      shell:
          "ldsc.py --h2 {input.gwasf} --ref-ld-chr {params.annot},{params.baseline} --w-ld-chr {params.weights} --thin-annot --overlap-annot --frqfile-chr {params.frq} --out {params.odir} "




  rule run_ldsc_cor:
      input:
          unpack(get_annot_pairs)
      output:
          dataf="{gwasA},{gwasB}/{anno_name}.log"
      params:
          annot=lambda wildcards: ','.join(expand(config_d['L2']+"{anno_name}.",anno_name=all_annot['ptb_model'][wildcards.anno_name])),
          baseline=config_d['L2']+"baseline/baselineLD.",
          weights=config_d['WEIGHTS']+"weights.hm3_noMHC.",
          frq=config_d['FRQF'] +"1000G.EUR.QC.",
          odir="{gwasA},{gwasB}/{anno_name}"
      conda:
          config_e['ldsc']
      shell:
          "ldsc.py --rg {input.gwasfA},{input.gwasfB} --ref-ld-chr {params.annot},{params.baseline} --w-ld-chr {params.weights} --thin-annot --overlap-annot --frqfile-chr {params.frq} --out {params.odir} "


  # rule run_ldsc:
  #     input:
  #         anno_ld=expand(config_d["L2"]+"new_baseline/{{anno_name}}.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
  #         baselinef=  expand(config_d['WEIGHTS'] +"weights.hm3_noMHC.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
  #         gwasf=config_d['GWAS'] +"ldsc_input/ptb_gwas.sumstats.gz"
  #     output:
  #         dataf="{anno_name}.results"
  #     log:
  #         tempf=temp("{anno_name}.log")
  #     params:
  #         annot=config_d["L2"]+"new_baseline/{anno_name}",
  #         weights=config_d['WEIGHTS']+"weights.hm3_noMHC.",
  #         frq=config_d['FRQF'] +"1000G.EUR.QC.",
  #         odir="{anno_name}"
  #     conda:
  #         config_e['ldsc']
  #     shell:
  #         """python2 ../ldsc.py --h2 {input.gwasf} --ref-ld-chr {params.annot} --w-ld-chr {params.weights} --overlap-annot --frqfile-chr {params.frq} --out {params.odir} """




#+END_SRC


#+END_SRC


** Running Torus

The input that torus accepts is very similar to stratified LD score regression.  The main difference is torus 
