** Config

I start off by defining some absolute directories that will be referred to throughout the script.  Hopefully I only have to change these 

#+BEGIN_SRC yaml :tangle ../workflow/config_base.yaml
---
'DL': "/project2/xinhe/"
'1KG': "1kg/"
'BED': "genomic_annotation/ptb_epigenetic/"
'ANNO': "LDSC_Annotations/"
'L2': "genomic_annotation/L2/"
'WEIGHTS': "1kg/1000G_Phase3_weights_hm3_no_MHC/"
'FRQF': "1kg/1000G_Phase3_frq/"
'GWAS': "ptb/"
#+END_SRC


#+BEGIN_SRC yaml :tangle ../workflow/config_base_gardner.yaml
---
'DL': "/gpfs/data/xhe-lab/"
'1KG': "1kg/"
'BED': "genomic_annotation/ptb_epigenetic/"
'ANNO': "LDSC_Annotations/"
'L2': "genomic_annotation/L2/"
'WEIGHTS': "1kg/1000G_Phase3_weights_hm3_no_MHC/"
'FRQF': "1kg/1000G_Phase3_frq/"
'GWAS': "ptb/"
#+END_SRC


#+BEGIN_SRC yaml :tangle ../workflow/config.yaml
'DL': "/run/media/nwknoblauch/Data/"
'1KG': "/run/media/nwknoblauch/Data/1kg/"
'BED': "/run/media/nwknoblauch/Data/ptb_scratch/new_bed/"
'ANNO': "/run/media/nwknoblauch/Data/LDSC_Annotations/"
'L2': "/run/media/nwknoblauch/Data/L2/"
'WEIGHTS': "/run/media/nwknoblauch/Data/weights_hm3_no_hla/"
'FRQF': "/run/media/nwknoblauch/Data/1000G_frq/"
'GWAS': "/run/media/nwknoblauch/Data/gwas_data/gwas_sumstats/"
#+END_SRC

#+BEGIN_SRC yaml :tangle ../workflow/annots.yaml
  ---
  full: [
  'chip-seq-pooled-DSC1-dec-H3K27ac',
  'atac-seq-pooled-DSC2-dec-ATAC',
  'chip-seq-pooled-DSC1-ctr-H3K4me3',
  'atac-seq-pooled-DSC3-ctr-ATAC',
  'chip-seq-pooled-DSC3-ctr-H3K4me1',
  'chip-seq-pooled-DSC1-ctr-H3K4me1',
  'atac-seq-pooled-DSC1-ctr-ATAC',
  'chip-seq-reproducible-ctr-H3K4me3',
  'chip-seq-reproducible-dec-H3K27ac',
  'chip-seq-reproducible-dec-H3K4me3',
  'chip-seq-reproducible-dec-H3K4me1',
  'chip-seq-reproducible-ctr-H3K4me1',
  'atac-seq-reproducible-dec-ATAC',
  'chip-seq-reproducible-ctr-H3K27ac',
  'atac-seq-reproducible-ctr-ATAC',
  'chip-seq-dec_up-H3K4me1',
  'chip-seq-pooled-DSC3-dec-H3K4me3',
  'atac-seq-pooled-DSC2-ctr-ATAC',
  'chip-seq-dec_up-H3K4me3',
  'chip-seq-pooled-DSC3-ctr-H3K4me3',
  'atac-seq-dec_down-ATAC',
  'atac-seq-pooled-DSC3-dec-ATAC',
  'chip-seq-dec_up-H3K27ac',
  'chip-seq-dec_down-H3K4me3',
  'chip-seq-pooled-DSC2-ctr-H3K4me1',
  'chip-seq-pooled-DSC1-dec-H3K4me1',
  'chip-seq-pooled-DSC2-ctr-H3K27ac',
  'chip-seq-pooled-DSC2-dec-H3K4me1',
  'chip-seq-pooled-DSC1-dec-H3K4me3',
  'atac-seq-dec_up-ATAC',
  'chip-seq-pooled-DSC2-dec-H3K27ac',
  'chip-seq-pooled-DSC3-dec-H3K27ac',
  'chip-seq-dec_down-H3K27ac',
  'chip-seq-pooled-DSC3-ctr-H3K27ac',
  'chip-seq-dec_down-H3K4me1',
  'chip-seq-pooled-DSC3-dec-H3K4me1',
  'chip-seq-pooled-DSC2-dec-H3K4me3',
  'chip-seq-pooled-DSC2-ctr-H3K4me3',
  'atac-seq-pooled-DSC1-dec-ATAC',
  'chip-seq-pooled-DSC1-ctr-H3K27ac']
  reproducible: [
  'chip-seq-reproducible-ctr-H3K4me3',
  'chip-seq-reproducible-dec-H3K27ac',
  'chip-seq-reproducible-dec-H3K4me3',
  'chip-seq-reproducible-dec-H3K4me1',
  'chip-seq-reproducible-ctr-H3K4me1',
  'atac-seq-reproducible-dec-ATAC',
  'chip-seq-reproducible-ctr-H3K27ac',
  'atac-seq-reproducible-ctr-ATAC'
  ]

#+END_SRC

#+BEGIN_SRC snakemake :tangle ../workflow/snakefile

  import yaml

    # configfile:
    #     'config_base.yaml'

  with open("../workflow/config_base_gardner.yaml") as stream:
        config=yaml.safe_load(stream)

  config_base = config.pop('DL')

  config_d = {x:(config_base+y) for x,y in config.items()}
  config_d['DL']=config_base



  def get_annot_files(wildcards):
      return {'anno_l2':expand(config_d['L2'] +"{anno_name}.{chrom}.l2.ldscore.gz",chrom=range(1,23),anno_name=all_annot[wildcards.anno_name]),
              'baseline_l2':expand(config_d['L2'] +"baseline/baselineLD.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
              'gwasf':config_d['GWAS'] +"ldsc_input/ptb_gwas.sumstats.gz",
              'baselinef':  expand(config_d['WEIGHTS'] +"weights.hm3_noMHC.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
              'freqf':  expand(config_d['FRQF'] +"1000G.EUR.QC.{chrom}.frq",chrom=range(1,23)),
      }



  include: "dl_snakefile"
  include: "gwas_snakefile"
  with open("annots.yaml", 'r') as stream:
      all_annot = yaml.safe_load(stream)
      #(all_annot)
  wildcard_constraints:
      chrom="\d+"

  localrules: all, get_hm3_snplist,get_plinkfiles,get_frq,get_weights

  rule all:
      input:
          config_d['GWAS'] +"ldsc_input/ptb_gwas.sumstats.gz",
            "reproducible.results"



#+END_SRC

** Downloading files

The first step is to download some LD score regression stuff from the web. In particular we want a gzipped tarball of the hapmap 3 SNPs.

#+BEGIN_SRC snakemake :tangle ../workflow/dl_snakefile
rule get_hm3_snplist:
    output:
        temp(config_d['DL'] +"hapmap3_snps.tgz")
    shell:
        "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/hapmap3_snps.tgz -O {output}"
#+END_SRC

Next we'll unzip the files and put them somewhere on disk.

#+BEGIN_SRC snakemake :tangle ../workflow/dl_snakefile

rule gunzip_hm3:
    input:
        rules.get_hm3_snplist.output
    params:
        dld=config_d['1KG']
    output:
        expand(config_d['1KG']+"hapmap3_snps/"+"hm.{chrom}.snp",chrom=range(1,23))
    shell:
        "tar -C {params.dld} -xvzf {input}"


#+END_SRC

The rsids don't come with coordinates, and we don't have coordinates for our GWAS data, so we'll use the ~SNPlocs.Hsapiens.dbSNP144.GRCh37~ package 
to get the coordinates corresponding to these rsids.  Also note that we won't be able to get all of them, as some rsids have been merged by NCBI.

#+BEGIN_SRC R :tangle ../scripts/rsid2loc.R

  library(tidyverse)
  library(ldmap)


  input_f <- snakemake@input[["input"]]
  output_f <- snakemake@output[["output"]]
  input_ids <- EigenH5::fast_str2int(scan(input_f, what = character()), prefix = "rs")
  input_ids <- input_ids[!is.na(input_ids)]
  BSgenome::snpsById(SNPlocs.Hsapiens.dbSNP144.GRCh37::SNPlocs.Hsapiens.dbSNP144.GRCh37,
                     ids = input_ids,
                     ifnotfound = "warn") %>% as_tibble() %>% 
      dplyr::rename(chrom = seqnames, rsid = RefSNP_id) %>%
      dplyr::mutate(chrom = as.integer(chrom),
                    rsid = rsid) %>%
      select(-strand) %>%
      readr::write_tsv(output_f)

#+END_SRC

#+RESULTS:

#+BEGIN_SRC snakemake :tangle ../workflow/dl_snakefile

  # rule snp2coord:
  #     input:
  #         inputf=config_d['1KG']+"hapmap3_snps/"+"hm.{chrom}.snp"
  #     output:
  #         outputf=config_d['1KG']+"hapmap3_snps/"+"hm.{chrom}.tsv.gz"
  #     script:
  #         "../scripts/rsid2loc.R"
    
#+END_SRC

** Munging the GWAS data

Unfortunately I don't have a remote source for the gwas summary statistics I can point you to, so we'll just pretend like you know
how to get to `meta.stat` the PTB gwas file.  First thing is to convert it to HDF5 for easier read/write of subsets


#+BEGIN_SRC R :tangle ../scripts/gwas2h5.R

  library(tidyverse)
  library(EigenH5)
  library(readr)
  library(ldmap)


  mc <- cols(
      rsid = col_character(),
      chrom = col_integer(),
      pos = col_double(),
      A1 = col_character(),
      A2 = col_character(),
      N = col_double(),
      freq = col_double(),
      beta = col_double(),
      se = col_double(),
      pval = col_double(),
      Q = col_double(),
      het = col_double(),
      N.local = col_double(),
      freq.local = col_double(),
      beta.local = col_double(),
      se.local = col_double(),
      pval.local = col_double(),
      N.23andMe = col_double(),
      freq.23andMe = col_double(),
      beta.23andMe = col_double(),
      se.23andMe = col_double(),
      pval.23andMe = col_double()
  )


  input_f <- snakemake@input[["inputf"]]
  output_f <- snakemake@output[["outputf"]]


  callback_fun <- function(df, filename, datapath, ...){
    write_df_h5(
      df = dplyr::slice(
                    dplyr::mutate(df,
                                  ref = fast_str2ascii(A2),
                                  alt = fast_str2ascii(A1),
                                  snp_struct =
                                    new_ldmap_snp(chrom, pos, ref, alt),
                                  rsid = fast_str2int(rsid, prefix = "rs"),
                                  ),
                    rank.ldmap_snp(snp_struct)),
      filename = filename, datapath = datapath, ... = ...)
  }

  stopifnot(!is.null(input_f),
            !is.null(output_f),
            file.exists(input_f),
            !file.exists(output_f))

  delim2h5(input_f,
           output_file = output_f,
           h5_args = list(datapath = "snp"),
           delim = "\t",
           col_names = names(mc$cols),
           skip = 1L,
           callback_fun = callback_fun,
           col_types = mc,
           progress = TRUE,
           chunk_size = 150000)

  chrom_vec <- read_vector_h5v(output_f, "snp/chrom", i = integer())
  chrom_df <- rle2offset(chrom_vec) %>%
      dplyr::rename(chrom = value)
  write_df_h5(chrom_df,output_f,"chrom_offset")
#+END_SRC




#+BEGIN_SRC snakemake :tangle ../workflow/gwas_snakefile

  rule ptb_gwas2h5:
      input:
          inputf=config_d['GWAS']+"meta.stat"
      output:
          outputf=config_d['GWAS'] +"ptb_gwas.h5"
      conda:
          "../envs/eigenh5.yml"
      script:
          "../scripts/gwas2h5.R"

#+END_SRC



Next is to write some code to pull out the indices with the matching rsids (using coordinates, not rsid)


#+BEGIN_SRC R :tangle ../scripts/index_gwas.R

    library(tidyverse)
    library(EigenH5)
    library(readr)
    library(ldmap)
    ## load("~/Dropbox/Repos/ldsc/workflow/tf.RData")

    input_f <- snakemake@input[["inputf"]]
    index_f <-  snakemake@input[["indexf"]]
    chrom <- snakemake@params[["chrom"]]
    stopifnot(!is.null(chrom))
    schrom <- as.integer(chrom)
    output_f <- snakemake@output[["outputf"]]


    ind_spec <- cols_only(
      CHR = col_integer(),
      BP = col_double(),
      SNP = col_character()
    )

    gwas_type <- if_else(
      is.null(snakemake@params[["gwas_t"]]),
      "",
      paste0(".", snakemake@params[["gwas_t"]])
    )


    beta_col <- glue::glue("beta{gwas_type}")
    se_col <- glue::glue("se{gwas_type}")
    N_col <- glue::glue("N{gwas_type}")
    P_col <- glue::glue("pval{gwas_type}")

    sel_cols <- c("snp_struct",
                  beta_col,
                  "A1",
                  "A2",
                  se_col,
                  N_col,
                  P_col)

    sel_cols <- stringr::str_replace(
                           sel_cols,
                           "\\.$",
                           "")

    index_df <- vroom::vroom(
                         index_f,
                         delim = "\t",
                         col_types = ind_spec
                       )  %>% 
      rename(chrom=CHR,rsid=SNP,pos=BP)
      nr_index_df <- nrow(index_df)

    chrom_df <- read_tibble_h5(input_f, "chrom_offset", list()) %>%
      filter(chrom == schrom) %>% mutate(offset = as.integer(offset), datasize = as.integer(datasize)) %>% 
      arrange(offset)

    jdf <- pmap_dfr(chrom_df, function(chrom, datasize, offset) {
  #    subset_l <- seq(offset + 1, length.out = datasize)
      input_i <- EigenH5::read_df_h5(filename = input_f,
                              datapath = "snp",
                                subcols = sel_cols,
                                offset=offset,
                                datasize=datasize) %>%
        mutate(subset = (1:n()) + offset)

        inner_join(index_df,  bind_cols(input_i,ldmap::ldmap_snp_2_dataframe(input_i$snp_struct)))
    })

                                          #%>% mutate(snp_struct = as_ldmap_snp(snp_struct))  %>%
  stopifnot(all(jdf$chrom == schrom))
  stopifnot(nrow(jdf) == nr_index_df)

    jdf  %>% rename(beta =  {{beta_col}},
                    se =  {{se_col}},
                    N =  {{N_col}}) %>%
      dplyr::distinct(rsid, .keep_all = TRUE) %>% 
      dplyr::transmute(SNP = paste0("rs",rsid), N = N, Z = beta / se, A1 = A1, A2 = A2,P=pval) %>%
      vroom::vroom_write(output_f,delim = "\t")
#+END_SRC

#+BEGIN_SRC R :tangle ../scripts/gen_ldsc_sumstats.R
library(vroom)
library(magrittr)

 input_f <- snakemake@input[["inputf"]]
 output <- snakemake@output[["outputf"]]

 vroom::vroom(input_f,delim="\t") %>% vroom_write(output,delim="\t")


#+END_SRC





#+BEGIN_SRC snakemake :tangle ../workflow/gwas_snakefile

  rule indexgwas2h5:
      input:
          inputf=config_d['GWAS'] +"ptb_gwas.h5",
          indexf=config_d['L2'] +"baseline/baselineLD.{chrom}.l2.ldscore.gz"
      params:
          chrom="{chrom}"
      output:
          outputf=temp(config_d['GWAS'] +"hm3_index/ptb_gwas_hm_chr{chrom}.tsv")
      conda:
          "../envs/eigenh5.yml"
      script:
          "../scripts/index_gwas.R"

  rule prep_ldsc_sumstsat:
      input:
          inputf=expand(config_d['GWAS'] +"hm3_index/ptb_gwas_hm_chr{chrom}.tsv",chrom=range(1,23))
      params:
          gwas_t=""
      output:
          outputf=temp(config_d['GWAS'] +"ldsc_input/pre_ptb_gwas.sumstats.gz")
      conda:
          "../envs/eigenh5.yml"
      script:
          "../scripts/gen_ldsc_sumstats.R"


  rule check_ldsc_sumstat:
      input:
          config_d['GWAS'] +"ldsc_input/pre_ptb_gwas.sumstats.gz"
      params:
          outputf=config_d['GWAS'] +"ldsc_input/ptb_gwas"
      conda:
          "../envs/ldsc.yml"
      output:
          outputf=config_d['GWAS'] +"ldsc_input/ptb_gwas.sumstats.gz",
      log:
          logf=config_d['GWAS'] +"ldsc_input/ptb_gwas.log"
      shell:
          "python2 ../munge_sumstats.py --sumstats {input} --out {params.outputf}"
#+END_SRC

#+BEGIN_SRC bash :session rcc2 :dir /ssh:rcc2:/project2/xinhe/software/ldsc/workflow/
. "/project2/xinhe/software/miniconda3/etc/profile.d/conda.sh"
conda activate cause_gwas
snakemake -n


#+END_SRC

** Running LDSC

#+BEGIN_SRC snakemake :tangle ../workflow/dl_snakefile

  rule get_baseline_model:
      output:
          temp(config_d['DL']+"1000G_Phase3_baselineLD_v2.2_ldscores.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_baselineLD_v2.2_ldscores.tgz -O {output}"

  rule get_weights:
      output:
          temp(config_d["DL"]+"1000G_Phase3_weights_hm3_no_MHC.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_weights_hm3_no_MHC.tgz -O {output}"

  rule gunzip_weights:
      input:
          config_d["DL"]+"1000G_Phase3_weights_hm3_no_MHC.tgz"
      output:
          ldfiles = expand(config_d['WEIGHTS'] +"weights.hm3_noMHC.{chrom}.l2.ldscore.gz",chrom=range(1,23))
      params:
          W=config_d['1KG']
      shell:
          "tar -xvzf {input} -C {params.W}"        
        
  rule get_frq:
      output:
          temp(config_d['DL']+"1000G_Phase3_frq.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_frq.tgz -O {output}"


  rule get_plinkfiles:
      output:
          temp(config_d['DL'] +"1000G_Phase3_plinkfiles.tgz")
      shell:
          "wget https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_plinkfiles.tgz -O {output}"



  rule gunzip_plinkfiles:
      input:
          config_d['DL'] +"1000G_Phase3_plinkfiles.tgz"
      output:
          fam_files = expand(config_d['1KG'] +"1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.fam",chrom=range(1,23)),
          bim_files = expand(config_d['1KG'] +"1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bim",chrom=range(1,23)),
          bed_files = expand(config_d['1KG'] +"1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bed",chrom=range(1,23))
      params:
          KG=config_d['1KG']
      shell:
          "tar -xvzf {input} -C {params.KG}"

  rule gunzip_frqf:
      input:
          config_d['DL'] +"1000G_Phase3_frq.tgz"
      output:
          fam_files = expand(config_d['FRQF'] +"1000G.EUR.QC.{chrom}.frq",chrom=range(1,23)),
      params:
          KG=config_d['1KG']
      shell:
          "tar -xvzf {input} -C {params.KG}"


  rule gunzip_baseline:
      input:
          config_d['DL'] +"1000G_Phase3_baselineLD_v2.2_ldscores.tgz"
      output:
          ldfiles = expand(config_d['L2'] +"baseline/baselineLD.{chrom}.l2.ldscore.gz",chrom=range(1,23)),
          annotf = expand(config_d['L2'] +"baseline/baselineLD.{chrom}.annot.gz",chrom=range(1,23)),
          m50 = expand(config_d['L2'] +"baseline/baselineLD.{chrom}.l2.M_5_50",chrom=range(1,23)),       
      params:
          L2=config_d['L2']
      shell:
          "tar -xvzf {input} -C {params.L2}/baseline"



  rule unzip_annot:
      input:
          config_d['BED'] + "{annot}.bed.bz2"
      output:
          temp(config_d['BED'] + "{annot}.bed")
      shell:
          "bzip2 -cd {input} > {output}"


  rule make_annot:
      input:
          anno_bed=config_d['BED'] +"{annot}.bed",
          bim=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bim"
      output:
          annot = config_d['ANNO'] +"{annot}/{annot}.{chrom}.annot.gz"
      params:
          anno_name='{annot}'
      conda:
          "../envs/ldsc.yml"
      shell:
          "python2 ../make_annot.py --bed-file {input.anno_bed} --bimfile {input.bim} --annot-file {output.annot}"



  rule cmp_ldscores:
      input:
          anno_bed=config_d['ANNO'] +"{annot}/{annot}.{chrom}.annot.gz",
          bim=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bim",
          bed=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.bed",
          fam=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}.fam"
      output:
          tempf=temp(config_d['L2']+"{annot}.{chrom}.log"),
          l2=config_d['L2']+"{annot}.{chrom}.l2.M",
          l2M_50=config_d['L2']+"{annot}.{chrom}.l2.M_5_50",
          l2gz=config_d['L2']+"{annot}.{chrom}.l2.ldscore.gz"
      params:
          plink=config_d['1KG'] + "1000G_EUR_Phase3_plink/1000G.EUR.QC.{chrom}",
          odir=config_d['L2']+"{annot}.{chrom}"
      conda:
          "../envs/ldsc.yml"
      shell:
          "python2 ../ldsc.py --l2 --bfile {params.plink} --ld-wind-cm 1 --annot {input.anno_bed} --thin-annot --out {params.odir} "

  # def ldsc_fun(wildcards):
  #     {tchrom: expand(config_d['L2'] +"{anno_name}.{chrom}.l2.ldscore.gz",chrom=[tchrom],anno_name=['baseline' *all_annot[wildcards.anno_name]]) for tchrom in range(1,23)}

  # rule check_ldsc:
  #     input:
  #         unpack(ldsc_fun)
  #     output:
  #         temp("{anno_name}.check")
  #     script:
  #         "../scripts/check_ldscfiles.R"
#+END_SRC

#+BEGIN_SRC R :tangle ../scripts/check_ldscfiles.R

  library(vroom)
  library(tidyverse)
  library(fs)

  file_list <- snakemake@input

  spec <- cols(
    CHR = col_double(),
    SNP = col_character(),
    BP = col_double(),
    L2 = col_skip()
  )

  spec_base <- cols(
    CHR = col_double(),
    SNP = col_character(),
    BP = col_double(),
    CM = col_double(),
    MAF = col_double(),
    base = col_double(),
    Coding_UCSC = col_double(),
    Coding_UCSC.extend.500 = col_double(),
    Conserved_LindbladToh = col_double(),
    Conserved_LindbladToh.extend.500 = col_double(),
    CTCF_Hoffman = col_double(),
    CTCF_Hoffman.extend.500 = col_double(),
    DGF_ENCODE = col_double(),
    DGF_ENCODE.extend.500 = col_double(),
    DHS_peaks_Trynka = col_double(),
    DHS_Trynka = col_double(),
    DHS_Trynka.extend.500 = col_double(),
    Enhancer_Andersson = col_double(),
    Enhancer_Andersson.extend.500 = col_double(),
    Enhancer_Hoffman = col_double(),
    Enhancer_Hoffman.extend.500 = col_double(),
    FetalDHS_Trynka = col_double(),
    FetalDHS_Trynka.extend.500 = col_double(),
    H3K27ac_Hnisz = col_double(),
    H3K27ac_Hnisz.extend.500 = col_double(),
    H3K27ac_PGC2 = col_double(),
    H3K27ac_PGC2.extend.500 = col_double(),
    H3K4me1_peaks_Trynka = col_double(),
    H3K4me1_Trynka = col_double(),
    H3K4me1_Trynka.extend.500 = col_double(),
    H3K4me3_peaks_Trynka = col_double(),
    H3K4me3_Trynka = col_double(),
    H3K4me3_Trynka.extend.500 = col_double(),
    H3K9ac_peaks_Trynka = col_double(),
    H3K9ac_Trynka = col_double(),
    H3K9ac_Trynka.extend.500 = col_double(),
    Intron_UCSC = col_double(),
    Intron_UCSC.extend.500 = col_double(),
    PromoterFlanking_Hoffman = col_double(),
    PromoterFlanking_Hoffman.extend.500 = col_double(),
    Promoter_UCSC = col_double(),
    Promoter_UCSC.extend.500 = col_double(),
    Repressed_Hoffman = col_double(),
    Repressed_Hoffman.extend.500 = col_double(),
    SuperEnhancer_Hnisz = col_double(),
    SuperEnhancer_Hnisz.extend.500 = col_double(),
    TFBS_ENCODE = col_double(),
    TFBS_ENCODE.extend.500 = col_double(),
    Transcribed_Hoffman = col_double(),
    Transcribed_Hoffman.extend.500 = col_double(),
    TSS_Hoffman = col_double(),
    TSS_Hoffman.extend.500 = col_double(),
    UTR_3_UCSC = col_double(),
    UTR_3_UCSC.extend.500 = col_double(),
    UTR_5_UCSC = col_double(),
    UTR_5_UCSC.extend.500 = col_double(),
    WeakEnhancer_Hoffman = col_double(),
    WeakEnhancer_Hoffman.extend.500 = col_double()
  )

  file_list <- fs::dir_ls("/run/media/nwknoblauch/Data/L2",regexp =  ".+\\.([0-9]+)\\.l2.ldscore.gz$")
  l2chrom <- str_replace(file_list,".+\\.([0-9]+)\\.l2.ldscore.gz$","\\1")

  file_df <- tibble(path =file_list,chrom = l2chrom) 

  ct_df <- pmap_df(file_df,function(path,chrom) {
    tibble(rows = (vroom::vroom(path,delim = "\t",col_types = spec) %>% distinct() %>% nrow()),
           path = path,
           chrom = chrom)})

  baseline_f <- filter(file_df,str_detect(path,"baseline")) %>% rename(baseline_path = path)
  idf <- filter(file_df,str_detect(path,"baseline",negate = TRUE)) %>% distinct(chrom, .keep_all = TRUE) %>% inner_join(baseline_f) %>% mutate(newpath = str_replace(baseline_path,"baseline","new_baseline")) %>% arrange(as.integer(chrom))
  ## ctr  <- group_by(ct_df, chrom)  %>%
  ##   summarise(nrows = length(unique(rows)))
  pwalk(idf,function(path,chrom,baseline_path,newpath) {
    cat(path,",",baseline_path,"\n")
    tidf <- 
    semi_join(vroom::vroom(baseline_path,delim = "\t",col_types = spec_base),
              vroom::vroom(path,delim = "\t",col_types = spec)) %>% vroom::vroom_write(path = newpath,delim = "\t")
    })
  
  


#+END_SRC




#+BEGIN_SRC snakemake :tangle ../workflow/dl_snakefile

      rule run_ldsc:
          input:
              unpack(get_annot_files)
          output:
              dataf="{anno_name}.results"
          log:
              tempf=temp("{anno_name}.log")
          params:
            annot=lambda wildcards: ','.join(expand(config_d['L2']+"{anno_name}.",anno_name=all_annot[wildcards.anno_name])),
            baseline=config_d['L2']+"baseline/baselineLD.",
            weights=config_d['WEIGHTS']+"weights.hm3_noMHC.",
            frq=config_d['FRQF'] +"1000G.EUR.QC.",
            odir="{anno_name}"
          conda:
              "../envs/ldsc.yml"
          shell:
              """python2 ../ldsc.py --h2 {input.gwasf} --ref-ld-chr {params.annot},{params.baseline} --w-ld-chr {params.weights} --thin-annot --overlap-annot --frqfile-chr {params.frq} --out {params.odir} """






#+END_SRC


#+END_SRC
